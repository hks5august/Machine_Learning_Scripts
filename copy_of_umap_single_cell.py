# -*- coding: utf-8 -*-
"""Copy of UMAP-single cell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SxvD2L3IFpHK0y2OsSIVZuWT0Rbzsb14

# UMAP
Uniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data:

The data is uniformly distributed on a Riemannian manifold;
The Riemannian metric is locally constant (or can be approximated as such);
The manifold is locally connected.

From these assumptions it is possible to model the manifold with a fuzzy topological structure. The embedding is found by searching for a low dimensional projection of the data that has the closest possible equivalent fuzzy topological structure.

The details for the underlying mathematics can be found in the publication:
McInnes, L, Healy, J, [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426), ArXiv e-prints 1802.03426, 2018

The important thing is that you don’t need to worry about that—you can use UMAP right now for dimension reduction and visualisation as easily as a drop in replacement for scikit-learn’s t-SNE.

Documentation is available via Read the Docs.

https://pypi.org/project/umap-learn/
"""

pip install umap-learn

import os, umap
import pandas as pd

df = df = pd.read_csv('https://reneshbedre.github.io/assets/posts/tsne/ath_root_sub_seurat_processes.csv')
df = df.set_index(df.columns[0])
dft = df.T
dft = dft.set_index(dft.columns[0])

# check data
dft.head()

"""There are a number of parameters that can be set for the UMAP class; the major ones are as follows:

**n_neighbors:** This determines the number of neighboring points used in local approximations of manifold structure. Larger values will result in more global structure being preserved at the loss of detailed local structure. In general this parameter should often be in the range 5 to 50, with a choice of 10 to 15 being a sensible default.

**min_dist:** This controls how tightly the embedding is allowed compress points together. Larger values ensure embedded points are more evenly distributed, while smaller values allow the algorithm to optimise more accurately with regard to local structure. Sensible values are in the range 0.001 to 0.5, with 0.1 being a reasonable default.

**metric:** This determines the choice of metric used to measure distance in the input space. A wide variety of metrics are already coded, and a user defined function can be passed as long as it has been JITd by numba.
"""

# Define UMAP
brain_umap = umap.UMAP(random_state=999, n_neighbors=10, min_dist=.20)
# Fit UMAP and extract latent vars 1-2
embedding = pd.DataFrame(brain_umap.fit_transform(dft), columns = ['UMAP1','UMAP2'])

from sklearn.cluster import DBSCAN
# here eps parameter is very important and optimizing eps is essential
# for well defined clusters. I have run DBSCAN with several eps values
# and got good clusters with eps=3
dbscan_m = DBSCAN(eps=1, min_samples=10).fit(embedding)

import matplotlib.pyplot as plt 

plt.figure(figsize=(10, 10))

#Plot scatterplot for K-means Clustering
scatter = plt.scatter(embedding['UMAP1'],embedding['UMAP2'], c = dbscan_m.labels_, cmap ='rainbow')
plt.xlabel('TSNE1') 
plt.ylabel('TSNE2')
plt.legend(*scatter.legend_elements())

"""Now we can modify parameters to get better clustering results for DB scan or try to find a better representation of data"""

# Define UMAP
#(random_state=999, n_neighbors=10, min_dist=.20)
brain_umap = umap.UMAP(random_state=999, n_neighbors=10, min_dist=.25)
# Fit UMAP and extract latent vars 1-2
embedding = pd.DataFrame(brain_umap.fit_transform(dft), columns = ['UMAP1','UMAP2'])

#Previously we used: dbscan_m = DBSCAN(eps=1, min_samples=10).fit(embedding)
#key settings are: epsilon (eps) and min_samples
dbscan_m = DBSCAN(eps=2, min_samples=10).fit(embedding)

import matplotlib.pyplot as plt 

plt.figure(figsize=(10, 10))

#Plot scatterplot for K-means Clustering
scatter = plt.scatter(embedding['UMAP1'],embedding['UMAP2'], c = dbscan_m.labels_, cmap ='rainbow')
plt.xlabel('TSNE1') 
plt.ylabel('TSNE2')
plt.legend(*scatter.legend_elements())

pip install umap-learn[plot]

#How to use UMAP plotting feature:
#make sure you install umap.plot by : pip install umap-learn[plot]

import umap.plot

mapper = umap.UMAP().fit(dft)
umap.plot.points(mapper, labels=dbscan_m.labels_)